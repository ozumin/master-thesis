\chapter{手法}
本章では検討した解析方法について述べる．
\section{解析のアプローチ}
\subsection{分解能の決定}
ニューロンの活動データの扱いには時間分解能と空間分解能の2つの側面から検討する必要がある．
時間分解能については，蛍光強度データをそのまま用いる，時間窓に区切るなどが考えられる．
空間分解能については，ニューロン1個を見る場合，2個を見る場合，複数を見る場合が考えられる．
手法によってどのレベルでデータを扱うかが異なる．
\Tabref{tab:methods}にカルシウムイメージングデータを解析する際に使えそうな手法を載せる．

\begin{table}[htb]
  \center
  \begin{tabular}{|c|cc|} \hline
    & 生データ & 時間窓で区切る \\ \hline
    ペアで見る & 時系列クラスタリング & glasso，類似度+クラスタリング\\
	  複数で見る & 行列分解 & ロジスティック回帰，時系列クラスタリング \\ \hline
  \end{tabular}
  \caption{カルシウムイメージングデータ解析に使えそうな手法}
  \label{tab:methods}
\end{table}

本論文では，データに対して仮説をおいて数理モデルを立てた上で，行列分解を用いることにする．

\section{モデル}
カルシウムイメージングデータを解析するに当たって，いくつかの仮説をおいた．
\\ \\
\noindent \textbf{仮説 1}\\
グループが$K$個存在し，同じグループ内のニューロンは同時に活動する．
ニューロンは複数のグループに所属することができる．
観測時間内ではグループに属するニューロンは変化しない．
\\ \\
\textbf{仮説 2}\\
複数のグループが同時に活動する時，属するニューロンは被らない（ニューロンが属するグループは同時には活動しない）．
\\

これらの仮説をもとに，数理モデルを構築する．
ある行列$A$の$i$行を$a_{i:}$，$j$列を$a_{:j}$，$(i,j)$要素を$a_{ij}$または$[A]_{ij}$と表記する．
観測データ$X$はニューロン$I$個の観測時系列で，時系列の長さは$J$なので，$X \in \mathbb{R}_+^{I \times J}$である．
ニューロン$i$の観測時系列は$x_{i:} \in \mathbb{R}_+^{J}$である．

$c_{k:} \in \mathbb{R}^J_+$ ($k=1,\dots,K$)をグループ$k$の活動の時系列とすると，仮定より$x_{i:}$は$c_{i:}$の重み付き和として表す：
\begin{equation}
	x_{i:} = \sum_{k=1}^K d_{ik} c_{k:} + \eta_{i:},
  \label{eq:x}
\end{equation}
ただし，$d_{ik} \in \mathbb{R}^+$で，$\eta_{i:} \in \mathbb{R}^J$はガウスノイズの時系列である．
カルシウムイメージングのノイズはポアソン分布に従う光子ノイズであるが，光子数が多い場合はガウス分布で近似できる~\cite{Sjulson2007}．

$\mathbb{R}_+$を非負の実数の集合とする．
\eqref{eq:x}は行列形式で以下のように表現できる：
\begin{eqnarray}
  Y &=& DC, \\
  X &=& Y + H.
  \label{eq:model_matrix}
\end{eqnarray}
ただし，$D \in \mathbb{R}_+^{I \times K}$, $C \in \mathbb{R}_+^{K \times J}$, $H \in \mathbb{R}^{I \times J}$である．
また，$D$の要素$(i,k)$は$d_{ik}$，$C$の$i$行は$c_{i:}$，$H$の$i$行は$\eta_{i:}$である．

\subsection{NMF}
Nonnegative matrix factorization（NMF）\cite{Lee1999}は行列分解の手法の一つである．
NMFは以下の最適問題を解く：
\begin{equation}
	\argmin_{D \geq 0, C \geq 0} ||X - Y||_F^2.
  \label{eq:NMF}
\end{equation}

基底数$k$のNMFのモデルを$\mathcal{M}_k$とおく．
ノイズ行列$H$の各要素が正規分布$\mathcal{N} (0, \sigma^2)$に従う$\mathcal{M}_k$の尤度は以下である．
\begin{eqnarray}
	p(X | Y_k, \mathcal{M}_k) = \prod_{i,j} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{([Y_k]_{ij} - x_{ij})^2}{2 \sigma^2}),
\end{eqnarray}
ただし，$Y_k$はモデル$\mathcal{M}_k$における推定量である．
対数尤度は以下のようになる．
\begin{eqnarray}
	\log p(X | Y_k, \mathcal{M}_k) = - \frac{IJ}{2} (\log 2\pi + 2 \log \sigma) - \frac{1}{2 \sigma^2} \sum_{ij}([Y_k]_{ij} - x_{ij})^2.
\end{eqnarray}

\subsection{寄与率}
NMFの寄与率行列$P \in \mathbb{R}^{I \times K}$の要素を次のように定義する：
\begin{eqnarray}
	p_{ik} &=& \frac{||d_{ik} c_{k:}||_1}{\sum_{l=1}^K || d_{il} c_{l:} ||_1} \\
	&=& \frac{d_{ik} || c_{k:} ||_1}{ \sum_{l=1}^K d_{il} || c_{l:} ||_1 }.
\end{eqnarray}
要素$p_{ik}$はニューロン$i$に対する基底$k$の寄与率という意味である．

NMFの推定には一意性がなく，ある正則行列$Q$を考えた時，
\begin{eqnarray}
	X &=& DC \\
	&=& D Q R C \\
	&=& D'C', \\
	R &=& Q^{-1}, \\
	D' &=& DQ, \\
	C' &=& RC,
	\label{eq:dqrc}
\end{eqnarray}
のように別の$D'$と$C'$が推定される可能性がある．

この場合の$D'$と$C'$を用いて作られる寄与率行列$P'$と元の寄与率行列$P$の関係を考える．\textcolor{red}{これ以降長すぎ，行和1の制約前提で話してもいいかも}
要素$p'_{ik}$の分子は，$D'$と$C'$が正であることを用いて
\begin{eqnarray}
	d'_{ik} ||c'_{k:}||_1 &=& \left( \sum_{l = 1}^K d_{il} q_{lk} \right) \left( \sum_{m = 1}^K r_{km} ||c_{m:}||_1 \right) \\
	&=& \sum_{m = 1}^K \left( d_{i1} q_{1k} r_{km} + \cdots + d_{iK} q_{Kk} r_{km} \right) ||c_{m:}||_1,
\end{eqnarray}
である．
分母を考えると，
\begin{eqnarray}
	\sum_{l=1}^K d'_{il}||c'_{l:} ||_1 &=& \sum_{l = 1}^K \sum_{m = 1}^K \left( d_{i1} q_{1l} r_{lm} + \cdots + d_{iK} q_{Kl} r_{lm} \right) ||c_{m:}||_1 \\
	&=& \sum_{m = 1}^K \left( d_{i1} \sum_{l = 1}^K q_{1l}r_{lm} + \cdots d_{iK} \sum_{l = 1}^K q_{Kl} r_{lm}\right) ||c_{m:}||_1
\end{eqnarray}
\eqref{eq:dqrc}より，
\begin{eqnarray}
	\sum_{l = 1}^{K} q_{il} r_{lj} = \begin{cases}
    1 & (i = j) \\
    0 & (otherwise)
  \end{cases}
\end{eqnarray}
なので，
\begin{eqnarray}
	\sum_{l=1}^K d'_{il} || c'_{l:} ||_1 &=& \sum_{m = 1}^K d_{im} ||c_{m:}||_1,
\end{eqnarray}
となり，$p_{ik}$と$p'_{ik}$の分母は一致する．
寄与率$p'_{ik}$は，
\begin{eqnarray}
	p'_{ik} &=& \frac{ d'_{ik} || c'_{k:}||_1 }{ \sum_{m = 1}^K d'_{im} || c'_{m:}||_1 } \\
	&=& \frac{\sum_{l = 1}^K d_{il} q_{lk}}{\sum_{m=1}^K d_{im} || c_{m:} ||_1} ||c'_{k:}||_1 \\
	&=& ||c'_{k:}||_1 \sum_{l = 1}^K \frac{d_{il}}{\sum_{m=1}^K d_{im} || c_{m:} ||_1} q_{lk} \\
	p_{il} = \frac{d_{il} ||c_{l:}||_1 }{ \sum_{m = 1}^K d_{im} || c_{m:} ||_1 } \text{より，} \\
	&=& ||c'_{k:}||_1 \sum_{l = 1}^K \frac{p_{il}}{ ||c_{l:}||_1 } q_{lk}\\
	&=& \sum_{l = 1}^K \frac{ ||c'_{k:}||_1 }{ ||c_{l:}||_1 } q_{lk} p_{il}
\end{eqnarray}
となる．
これより，
\begin{eqnarray}
	P' &=& PV, \\
	V &\in& \mathbb{R}^{K \times K}, \\
	v_{ij} &=& \frac{ || c'_{j:} ||_1 }{|| c_{i:} ||_1} q_{ij},
\end{eqnarray}
と表せることから，$P'$は$P$を変換した行列であることがわかる．
% この変換によって異なるニューロンの寄与率は同じだけ変化する．
$X$に行和$1$の正規化を加え$C$に行和$1$の制約を加えると，$D$の行和も$1$となり，$P = D$，$V = Q$となる．

\subsection{推定量}
本論文における種々の解析では，基底数に寄らないNMFの推定量を用いる．
本研究で解決したい問題は，ニューロンがどのグループに所属するかというクラスタリングの問題である．
これは言い換えると，ニューロン同士が同じグループに所属するか否かの問題である．

用いる推定量$A \in [0, 1]^{I \times I}$は寄与率行列$P$から作成する．
まず，以下の手順で$P$と同じサイズの行列$G \in \{0, 1\}^{I \times K}$を作る．
$p_{i:}$の累積寄与率を$F_i$とおく．
$F_i$は$p_{i:}$を大きい順に足した関数である．
閾値を$threshold$とおき，$F_i < threshold$となる$p_{i:}$のインデックスを$index$とおく．
$p_{i:}$に該当する$g_{i:}$を
\begin{eqnarray}
	g_{ij} = \begin{cases}
		1 & (j \in index) \\
		0 & (otherwise)
	\end{cases}
\end{eqnarray}
と定義する．

推定量$A$を
\begin{eqnarray}
	A = G G^T,
\end{eqnarray}
と定義する．

この推定量はcluster ensembleでも用いられているpairwise similarity\cite{Boongoen2018}と似たものになっている．

また，$G$の作り方には一意性がないため推定量$A$にも一意性がない．

\subsection{NMFの基底数}
NMFの基底数の決め方にはいくつかのアプローチがある．
まずは，専門家や解析者の知識に基づいて決めることである．
この方法はデータに対して十分な知識がない時には使えない．

次にBIC~\cite{wasserman2000a}やAIC~\cite{Akaike1974}を用いる方法である．
これらは漸近理論に基づいた近似を行った情報量基準である．
NMFはデータが増えるほどパラメータ数$(I + J) * K$が増えるという特徴があり，これらを用いるのは本来不適切である．

次にRのNMFパッケージにも組み込まれているBrunetら~\cite{Brunet2004}の方法を紹介する．
彼らはNMFの推定結果からノード同士が同じ基底に所属するかしないかを表すconnectivity matrix $A \in \{0, 1\}^{I \times I}$を作成する．
本論文の推定量$A$と同じ意味の行列である．
初期値を20-100回変化させて$C$の平均$\bar{A} \in [0, 1]^{I \times I}$を計算する．
彼らは真の基底数ではこの推定量が$0$か$1$に寄るようになると仮定して，最も$\bar{A}$が安定する基底数を求める．
安定度は$1- \bar{A}$とそのcophenetic correlation coefficientのPearson correlationから計算する．

Ubaruら~\cite{Ubaru2017}はブートストラップを用いてNMFを行っており，$D$がそれほど変わらない基底数を採用している．
推定した$D_b$同士の相互相関行列についてdissimilarity~\cite{Wu}を測りその平均が最小となる基底数を用いる．

Hutchinsらはテストデータに対するRSS（residual sum of squares）が真の基底数以降になるとあまり下がらなくなると論じている\cite{Hutchins2008}．

Bayesian NMFではギブスサンプリングなどを用いてモデルエビデンスを計算している\cite{Cemgil2009}．

上記で述べた方法の他にも様々な方法が考案されているが，全てのデータに当てはめられるような枠組みは存在しない．

\subsection{バギングとモデル平均}
アンサンブル学習の一つにバギング\cite{Breiman1996}がある．
バギングはブートストラップ\cite{Efron1979}によって学習器を増やしその出力の平均をとる学習方法である．
ここでのブートストラップを$X$と同じ行列$X^b$を作成することとする．
NMFのブートストラップの方法には3種類考えられる：列のサンプリング，ブロックブートストラップ，残差型ブートストラップである．
列のサンプリングに夜ブートストラップは，データ行列$X$の各列がデータサンプルとしてみなせるので，列をサンプリングして$X^b$を作る方法である．
ブロックブートストラップでは，複数列を塊としてサンプリングする．
この方法は時系列データでのブートストラップで用いられており，時間方向に制約の入ったNMFでは有効である．
残差型ブートストラップは一回NMFのモデルを推定し，推定後の残差をサンプリングして推定した$DC$に足す方法である．
NMFのモデル\eqref{eq:model_matrix}では$H$はi.i.d.なノイズという仮定を置いているので，残差型ブートストラップが一番モデルに沿ったブートストラップ方法と言える．

バギングによってモデル$\mathcal{M}_k$から得られる推定量$A_k$は以下である：
\begin{eqnarray}
	A_k = \frac{1}{B} \sum_{b=1}^B A_k^b,
\end{eqnarray}
ただし，$B$はブートストラップサンプル数，$A_k^b$は$X^b$から推定された推定量である．

モデル平均とは異なるモデルの推定量の平均をとって精度向上を測る方法である．
本論文で用いる推定量$A$は基底数によって行列サイズが変化しないので，異なる基底数の推定結果の平均をとることができる．
その場合の推定量は：
\begin{eqnarray}
	A = \frac{1}{K_{max} - K_{min} + 1} \sum_{k=K_{min}}^{K_{max}} A_k,
\end{eqnarray}
ただし，平均する最小の基底数を$K_{min}$，最大の基底数を$K_{max}$とする．
NMFでは真の基底数を求めるのは難しい問題なので，異なる基底数の推定結果を平均して精度をそこまで落とさないようにする．
今回扱うデータから推定される$A$は基底数が違う時に大きくは変化しない．
真の基底数周りで平均した$A$の方が真の基底数でないモデルから推定された$A_k$よりも精度が落ちないと考えられる．

アンサンブル学習では，それぞれのモデルにある程度の推定精度があり\cite{Kittler1998}，答えに多様性がある方が精度が上がる\cite{Kuncheva2006}と言われている．
本論文でのモデル平均の使い方は，真の基底数が分からない時に推定結果をなまして推定を間違うリスクを下げるという使い方をしている．

\subsection{NMFのモデルエビデンス}
NMFの基底数を決める際にブートストラップから計算されたモデルエビデンスを用いることを考える．
BICは最尤推定量の尤度からモデルエビデンスを近似して扱っている．
ブートストラップによってモデルエビデンスを近似計算できると考えられる．
本節ではその説明をする．

今回の問題をBayesian model averagingの枠組みに当てはめると，
\begin{eqnarray}
	p(A|X) = \sum_k p(A|\mathcal{M}_k, X) p(\mathcal{M}_k | X)
\end{eqnarray}
で$p(A|X)$を求めることになる．
今回は異なる基底数のNMFから求まった$A$をモデルの事後確率で重み付けて足し合わせることになる．

モデルの事後確率は
\begin{eqnarray}
	p(\mathcal{M}_k|X) = \frac{m_k p(\mathcal{M}_k)}{\sum_l m_l p(M_l)}
\end{eqnarray}
で表される．
ただし，
\begin{eqnarray}
	m_k = \int p(X | Y_k, \mathcal{M}_k) p(Y_k| \mathcal{M}_k) dY_k
	\label{eq:evidence}
\end{eqnarray}
である．
これはモデルエビデンスやmarginal likelihoodと呼ばれる．
また，$p(\mathcal{M}_k)$はモデルが正しい確率である．

ある条件下で，母数の事後確率の密度関数はブートストラップによる最尤推定量の分布と同じになる．
そのため，ブートストラップサンプルから推定した$A$の分布をもとに事後確率を計算すれば良い．

現在の設定では$p(\mathcal{M}_k)$は一様なため，エビデンスを見る必要がある．

\subsubsection{ラプラス近似}
エビデンスの計算には事前分布を決めなければならず，積分も計算しなければいけない．
そこでラプラス近似により$m_k$は以下のような$\hat{m_k}$で近似できる．
\begin{eqnarray}
	\log \hat{m}_k = \log p(X | \hat{Y_k}, \mathcal{M}_k) - \frac{d_k}{2} \log n
	\label{eq:simm}
\end{eqnarray}
ここで，$d_k$は母数の数（$I \times K + K \times J$），$n$は観測データ数（$J$）である．
この近似を使ってログBayes因子を計算したのがBICである．

$\log p(X | \hat{Y_k}, \mathcal{M}_k)$は初期値を変えてNMFを行い，尤度が最も大きくなった対数尤度を用いる．
また，$\sigma^2 = Var(X - Y_k)$として計算する．

\subsubsection{ブートストラップによる近似}
ブートストラップの推定量の分布は最尤推定量の分布を近似する（ブートストラップサンプルが生成されたパラメータ分バイアスは乗る）ので，$m_k$は以下のように近似できる．

\begin{eqnarray}
	m_k &=& \int p(X | Y_k, \mathcal{M}_k) p(Y_k| \mathcal{M}_k) dY_k \\
	&\sim& \frac{1}{B} \sum_b \prod_{i,j} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{([Y_k^b]_{ij} - X^b_{ij})^2}{2 \sigma^2} \right) \\
	\label{eq:simm2}
\end{eqnarray}

ここで，$Y_k^b$はブートストラップサンプル$b$から計算された$Y_k$で，$B$はブートストラップサンプル数である．
また，$\sigma^2 = Var(X^b - Y^b_k)$として計算する．

ラプラス近似は本来NMFには不適切なので，ブートストラップによる近似でモデルエビデンスを計算することができれば基底数を推定できるかもしれない．
