\chapter{手法}
本章では検討した解析方法について述べる．
\section{解析のアプローチ}
\subsection{分解能の決定}
ニューロンの活動データの扱いには時間分解能と空間分解能の2つの側面から検討する必要がある．
時間分解能については，蛍光強度データをそのまま用いる，移動平均をとる，時間窓に区切るなどが考えられる．
空間分解能については，ニューロン1個を見る場合，2個を見る場合，複数を見る場合が考えられる．
手法によってどのレベルでデータを扱うかが異なる．
\Tabref{tab:methods}にカルシウムイメージングデータを解析する際に使えそうな手法を載せる．

\begin{table}[htb]
  \center
  \begin{tabular}{|c|cc|} \hline
    & 生データ & 時間窓で区切る \\ \hline
    ペアで見る & 時系列クラスタリング & glasso，類似度+クラスタリング\\
	  複数で見る & 行列分解 & ロジスティック回帰，時系列クラスタリング \\ \hline
  \end{tabular}
  \caption{カルシウムイメージングデータ解析に使えそうな手法}
  \label{tab:methods}
\end{table}

本論文では，データに対して仮説をおいて数理モデルを立てた上で，行列分解を用いることにする．

\section{モデル}
カルシウムイメージングデータを解析するに当たって，いくつかの仮説をおいた．
\\
\noindent \textbf{仮説 1}\\
グループが$K$個存在し，同じグループのニューロンは同時に活動する．
ニューロンは複数のグループに所属することができる．
観測時間内ではグループに属するニューロンは変化しない．\\
\textbf{仮説 2}\\
複数のグループが同時に活動する時，属するニューロンは被らない（ニューロンが属するグループは同時には活動しない）．
\\

これらの仮説をもとに，数理モデルを構築する．
$c_{k:}$, ($k=1,\dots,K$)をグループ$k$の活動の時系列とする．

ニューロン$i$の観測時系列を$x_{i:}$とおき，$c_{k:}$の重み付け和で表す：
\begin{equation}
	x_{i:} = \sum_{k=1}^K d_{ik} c_{k:} + \eta_{i:}.
  \label{eq:x}
\end{equation}
$\eta_{i:}$はガウスノイズの時系列である．
カルシウムイメージングのノイズはポアソン分布に従う光子ノイズであるが，光子数が多い場合はガウス分布で近似できる~\cite{Sjulson2007}．

$\mathbb{R}_+$を非負の実数の集合とする．
\Eqref{eq:x}は行列形式で以下のように表現できる：
\begin{eqnarray}
  Y &=& DC, \\
  X &=& Y + H.
  \label{model_matrix}
\end{eqnarray}
ただし，$X \in \mathbb{R}_+^{N \times T}$, $D \in \mathbb{R}_+^{N \times K}$, $C \in \mathbb{R}_+^{K \times T}$, $H \in \mathbb{R}^{N \times T}$である．
また，$X$の行は$x_i$，$D$の要素$(i,k)$は$d_{ik}$，$C$の行は$c_{i:}$，$H$の行は$\eta_{i:}$である．
これ以降，行列$A$の$i$行を$a_{i:}$，$j$列を$a_{:j}$，$(i,j)$要素を$a_{ij}$または$[A]_{ij}$と表記する．

\subsection{NMF}
Nonnegative matrix factorization（NMF）\cite{Lee1999}は行列分解の手法の一つである．
NMFは以下の目的関数を最小化する：
\begin{equation}
	\argmin_{D \geq 0, C \geq 0} ||X - Y||_F^2.
  \label{eq:NMF}
\end{equation}

基底数$k$のNMFのモデルを$\mathcal{M}_k$とおく．
ノイズ行列$H$の各要素が正規分布$\mathcal{N} (0, \sigma^2)$に従う$\mathcal{M}_k$の尤度は以下である．
\begin{eqnarray}
	p(X | Y_k, \mathcal{M}_k) = \prod_{i,j} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{([Y_k]_{ij} - x_{ij})^2}{2 \sigma^2})
\end{eqnarray}
対数尤度は以下のようになる．
\begin{eqnarray}
	\log p(X | Y_k, \mathcal{M}_k) = - \frac{IJ}{2} (\log 2\pi + 2 \log \sigma) - \frac{1}{2 \sigma^2} \sum_{ij}([Y_k]_{ij} - x_{ij})^2
\end{eqnarray}

\subsection{寄与率}
NMFの寄与率行列$P \in \mathbb{R}^{I \times K}$の要素を次のように定義する：
\begin{eqnarray}
	p_{ik} &=& \frac{||d_{ik} c_{k:}||_1}{\sum_{l=1}^K || d_{il} c_{l:} ||_1} \\
	&=& \frac{d_{ik} || c_{k:} ||_1}{ \sum_{l=1}^K d_{il} || c_{l:} ||_1 }.
\end{eqnarray}
要素$p_{ik}$はニューロン$i$に対する基底$k$の寄与率という意味である．

NMFの推定には一意性がなく，ある正則行列$Q$を考えた時，
\begin{eqnarray}
	X &=& DC \\
	&=& D Q R C \\
	&=& D'C', \\
	R &=& Q^{-1}, \\
	D' &=& DQ, \\
	C' &=& RC,
\end{eqnarray}
のように別の$D'$と$C'$が推定される可能性がある．

この場合の寄与率行列$P'$がどうなるかを考える．
要素$p'_{ik}$の分子は，$D'$と$C'$が正であることを用いて
\begin{eqnarray}
	d'_{ik} ||c'_{k:}||_1 &=& \left( \sum_{l = 1}^K d_{il} q_{lk} \right) \left( \sum_{m = 1}^K r_{km} ||c_{m:}||_1 \right) \\
	&=& \sum_{m = 1}^K \left( d_{i1} q_{1k} r_{km} + \cdots + d_{iK} q_{Kk} r_{km} \right) ||c_{m:}||_1,
\end{eqnarray}
である．
分母を考えると，
\begin{eqnarray}
	\sum_{l=1}^K d'_{il}||c'_{l:} ||_1 &=& \sum_{l = 1}^K \sum_{m = 1}^K \left( d_{i1} q_{1l} r_{lm} + \cdots + d_{iK} q_{Kl} r_{lm} \right) ||c_{m:}||_1 \\
	&=& \sum_{m = 1}^K \left( d_{i1} \sum_{l = 1}^K q_{1l}r_{lm} + \cdots d_{iK} \sum_{l = 1}^K q_{Kl} r_{lm}\right) ||c_{m:}||_1
\end{eqnarray}
定義より，
\begin{eqnarray}
	\sum_{l = 1}^{K} q_{il} r_{lj} = \begin{cases}
    1 & (i = j) \\
    0 & (otherwise)
  \end{cases}
\end{eqnarray}
なので，
\begin{eqnarray}
	\sum_{l=1}^K d'_{il} || c'_{l:} ||_1 &=& \sum_{m = 1}^K d_{im} ||c_{m:}||_1,
\end{eqnarray}
となり，$p_{ik}$と$p'_{ik}$の分母は一致する．
寄与率$p'_{ik}$は，
\begin{eqnarray}
	p'_{ik} &=& \frac{ d'_{ik} || c'_{k:}||_1 }{ \sum_{m = 1}^K d'_{im} || c'_{m:}||_1 } \\
	&=& \frac{\sum_{l = 1}^K d_{il} q_{lk}}{\sum_{m=1}^K d_{im} || c_{m:} ||_1} ||c'_{k:}||_1 \\
	&=& ||c'_{k:}||_1 \sum_{l = 1}^K \frac{d_{il}}{\sum_{m=1}^K d_{im} || c_{m:} ||_1} q_{lk} \\
	p_{il} = \frac{d_{il} ||c_{l:}||_1 }{ \sum_{m = 1}^K d_{im} || c_{m:} ||_1 } \text{より，} \\
	&=& ||c'_{k:}||_1 \sum_{l = 1}^K \frac{p_{il}}{ ||c_{l:}||_1 } q_{lk}\\
	&=& \sum_{l = 1}^K \frac{ ||c'_{k:}||_1 }{ ||c_{l:}||_1 } q_{lk} p_{il}
\end{eqnarray}
となる．
これより，
\begin{eqnarray}
	P' &=& PV, \\
	V &\in& \mathbb{R}^{K \times K}, \\
	v_{ij} &=& \frac{ || c'_{j:} ||_1 }{|| c_{i:} ||_1} q_{ij},
\end{eqnarray}
と表せることから，$P'$は$P$を変換した行列であることがわかる．
この変換によって異なるニューロンの寄与率は同じだけ変化する．

$X$，$D$，$C$にそれぞれ行和$1$の正規化を行うと，$P = D$，$V = Q$となる．

\subsection{推定量}
本論文では，基底数に寄らないNMFの推定量を用いる．
NMFで解決したい問題は，ニューロンがどのグループに所属するかというクラスタリングの問題である．
これを別の表現で表すと，ニューロン同士が同じグループに所属するかしないかということになる．

用いる推定量$A \in [0, 1]^{I \times I}$は寄与率行列$P$から作成する．
まず，$P$と同じサイズの行列$G \in \{0, 1\}^{I \times K}$を作る．

$p_{i:}$の累積寄与率を$F$とおく．
$F$は$p_{i:}$を大きい順に足した関数である．
閾値を$threshold$とおき，$F < threshold$となる$p_{i:}$のインデックスを$index$とおく．
$p_{i:}$に該当する$g_{ij} \ (j = 1, \cdots, K)$を
\begin{eqnarray}
	g_{ij} = \begin{cases}
		1 & (j \in index) \\
		0 & (otherwise)
	\end{cases}
\end{eqnarray}
と定義する．

推定量$A$を
\begin{eqnarray}
	A = G G^T,
\end{eqnarray}
と定義する．

この推定量はcluster ensembleでも用いられているpairwise similarity\cite{Boongoen2018}と同等である．

\subsection{NMFの基底数}
NMFの基底数の決め方にはいくつかのアプローチがある．
まずは，専門家や解析者の知識に基づいて決めることである．
この方法はデータに対して十分な知識がない時には使えない．

次にBIC\cite{wasserman2000a}やAIC\cite{Akaike1974}を用いる方法である．
これらは漸近理論に基づいた近似を行った情報量基準である．
NMFはデータが増えるほどパラメータ数$(I + J) * K$が増えるという特徴があり，これらを用いるのは本来不適切である．

次にRのNMFパッケージにも組み込まれているBrunetら\cite{Brunet2004}の方法を紹介する．
彼らはNMFの推定結果からノード同士が同じ基底に所属するかしないかを表すconnectivity matrix $C \in \{0, 1\}^{I \times I}$を作成する．
本論文の推定量$A$と同じ意味の行列である．
初期値を20-100回変化させて$C$の平均$\bar{C} \in [0, 1]^{I \times I}$を計算する．
彼らは真の基底数ではこの推定量が$0$か$1$に寄るようになると仮定して，最も$\bar{C}$が安定する基底数を求める．
安定度は$1- \bar{C}$とそのcophenetic correlation coefficientのPearson correlationから計算する．

\cite{Ubaru2017}ではブートストラップを用いてNMFを行っており，$C$がそれほど変わらない基底数を採用している．
行列間のdissimilarityを定義し，推定した$C_b$同士についてdissimilarityを測りその平均が最小となる基底数を用いる．

Hutchinsらはテストデータに対するRSS（residual sum of squares）が真の基底数以降になるとあまり下がらなくなると論じている\cite{Hutchins2008}．

Bayesian NMFではギブスサンプリングなどを用いてモデルエビデンスを計算している\cite{Cemgil2009}．

上記で述べた方法の他にも様々な方法が考案されていると思うが，全てのデータに当てはめられるような枠組みは存在しない．

\subsection{アンサンブル学習}
アンサンブル学習の一つにバギング\cite{Breiman1996}がある．
バギングはブートストラップ\cite{Efron1979}によって学習器を増やしその出力の平均をとる学習方法である．
NMFのブートストラップの方法には3種類考えられる：列のサンプリング，ブロックブートストラップ，残差型ブートストラップである．
データ行列$X$の各列が1つのデータとしてみなせるので，列をサンプリングして$X$と同じ大きさの行列を作るのが列のサンプリングによるブートストラップである．
ブロックブートストラップでは，複数列を塊としてサンプリングする．
この方法は時系列データでのブートストラップで用いられており，時間方向に制約の入ったNMFでは有効である．
残差型ブートストラップは一回NMFのモデルを推定し，推定後の残差をサンプリングして推定した$DC$に足す方法である．
NMFのモデル\ref{model_matrix}では$H$はi.i.d.なノイズという仮定を置いているので，残差型ブートストラップが一番モデルに沿ったブートストラップ方法と言える．

本論文で用いる推定量$A$は基底数に寄らないため，異なる基底数の推定結果の平均をとることができる．
アンサンブル学習では，それぞれのモデルにある程度の推定精度があり\cite{Kittler1998}，答えに多様性がある方が精度が上がる\cite{Kuncheva2006}と言われている．
しかし，今回のアンサンブル学習の使い方は，真の基底数が分からない時に推定結果をなまして推定を間違うリスクを下げるという使い方をしている．

\subsection{NMFのモデルエビデンス}
NMFの基底数を決める際にブートストラップから計算されたモデルエビデンスを用いることを考える．
BICは最尤推定量の尤度からモデルエビデンスを近似して扱っている．
ブートストラップによってモデルエビデンスを近似計算できると考えられる．
本節ではその説明をする．

Bayesian model averagingの枠組みに当てはめると，
\begin{eqnarray}
	p(A|X) = \sum_k p(A|\mathcal{M}_k, X) p(\mathcal{M}_k | X)
\end{eqnarray}
で$p(A|X)$を求めることになる．
今回は異なる基底数のNMFから求まった$A$をモデルの事後確率で重み付けて足し合わせることになる．

事後確率は
\begin{eqnarray}
	p(\mathcal{M}_k|X) = \frac{m_k p(\mathcal{M}_k)}{\sum_l m_l p(M_l)}
\end{eqnarray}
で表される．
ただし，
\begin{eqnarray}
	m_k = \int p(X | Y_k, \mathcal{M}_k) p(Y_k| \mathcal{M}_k) dY_k
	\label{eq:evidence}
\end{eqnarray}
である．
今回は$Y_k = D_k C_k$とおく，
これはmodel evidenceやmarginal likelihoodと呼ばれる．
また，$p(\mathcal{M}_k)$はモデルが正しい確率である．

ある条件下で，母数の事後確率の密度関数はブートストラップに対する最尤推定量の分布と同じになる．
そのため，ブートストラップサンプルから推定した$\bar A$の分布をもとに重みを計算すれば良い．
しかし，前回の実験では重みの有無で結果にあまり差異は見られなかった．

現在の設定では$p(\mathcal{M}_k)$は一様なため，エビデンスを見る必要がある．

\subsubsection{ラプラス近似}
エビデンスの計算には事前分布を決めなければならず，積分も計算しなければいけない．
そこでラプラス近似により$m_k$は以下のような$\hat{m_k}$で近似できる．
\begin{eqnarray}
	\log \hat{m}_k = \log p(X | \hat{Y_k}, \mathcal{M}_k) - \frac{d_k}{2} \log n
	\label{eq:simm}
\end{eqnarray}
ここで，$d_k$は母数の数（ニューロン数x基底数，基底数x時間），$n$は観測データ数（$X$の列数）である．
この近似を使ってBayes factorを計算したのがBICである．

$\log p(X | \hat{Y_k}, \mathcal{M}_k)$は初期値を変えてNMFを行い，尤度が最も大きくなった対数尤度を用いる．
また，$\sigma^2 = Var(X - Y_k)$とする．

\subsubsection{ブートストラップによる近似}
$Y_k$を$D_k$と$C_k$のパラメータとして，$m$を\Eqref{eq:simm}と近似した．
これから$Y_k = D_kC_k$として考える．
ブートストラップの推定量の分布は最尤推定量の分布を近似する（ブートストラップサンプルが生成されたパラメータ分バイアスは乗る）ので，\Eqref{eq:evidence}は以下のように近似できる．

\begin{eqnarray}
	m_k &=& \int p(X | Y_k, \mathcal{M}_k) p(Y_k| \mathcal{M}_k) dY_k \\
	&\sim& \frac{1}{B} \sum_b \prod_{i,j} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{([Y_k^b]_{ij} - X^b_{ij})^2}{2 \sigma^2} \right) \\
	\label{eq:simm2}
\end{eqnarray}

ここで，$Y_k^b$はブートストラップサンプル$b$から計算された$Y_k$で，$B$はブートストラップサンプル数である．
また，$\sigma^2 = Var(X^b - Y^b_k)$として計算している．
